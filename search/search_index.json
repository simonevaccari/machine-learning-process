{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Machine Learning Process","text":""},{"location":"#introduction","title":"Introduction","text":"<p>This learning resource demonstrates a machine learning system for classification of Sentinel-2 images into 10 different classes using cloud-native technologies. The system leverages MLFLOW to track the training process and select the best candidate trained model from MLFLOW server. </p> <p>There are two workflows developed one for training a deep learning model classifier on EuroSAT dataset and one for running prediction on a real world Sentinel-2 data.  The automation is achieved using Kubernetes-native tools, making the setup scalable, modular, and suitable for Earth observation and geospatial applications.</p>"},{"location":"#key-components","title":"Key Components","text":"<p>This setup integrates the following technologies and concepts:</p>"},{"location":"#mlflow","title":"MLFLOW","text":"<ul> <li>Manage end-to-end ML workflows, from development to production</li> <li>End-to-end MLOps solution for traditional ML, including integrations with traditional ML models, and Deep learning one.</li> <li>Simple, low-code performance tracking with autologging</li> <li>State-of-the-art UI for model analysis and comparison</li> </ul>"},{"location":"#stac-duckdb","title":"STAC &amp; DuckDB","text":"<ul> <li>Acts as the primary data source by providing geospatial data in a standardized format. The STAC collection includes references to the EUROSAT benchmark dataset, which can be queried using DuckDB.</li> </ul>"},{"location":"#high-level-architecture","title":"High-Level Architecture","text":"<p>The system is designed to handle the following flow:</p> <ol> <li> <p>Training pipeline: A CNN model trained on EuroSAT dataset which already exist on a dedicated STAC endpoint. The MLFLOW track the whole process to monitor the life cycle of training.</p> </li> <li> <p>Inference: Run the inference pipeline to perform tile-based classification on Sentinel-2 L1C products.</p> </li> <li> <p>Workflow Execution: Both training and inference pipeline will be executed using the CWL-based algorithm.</p> </li> </ol>"},{"location":"#why-use-this-setup","title":"Why Use This Setup?","text":"<p>This setup demonstrates the power of combining machine learning paradigms with container-native workflows to enable scalable geospatial analysis.</p> <p>It is particularly suited for Earth observation and scientific workflows because:</p> <ul> <li>Scalability: Kubernetes ensures workflows can handle varying loads effectively.</li> <li>Modularity: Components can be easily reused or replaced for other applications.</li> <li>Automation: Events trigger workflows without manual intervention, enabling real-time processing.</li> </ul> <p>Through this resource, you'll learn to implement a cloud-native pipeline for tile-based classification, which can be extended to other geospatial or scientific applications.</p>"},{"location":"hands-on/","title":"Hands-on","text":"<p>This page provides a step-by-step guide to help you run different components of the workflow:</p>"},{"location":"hands-on/#tile-based-training-implementation","title":"Tile-based Training Implementation","text":"<p>Refer to the training component documentation and run multiple training jobs with various model hyperparameter.</p>"},{"location":"hands-on/#tile-based-inference-implementation","title":"Tile-based Inference Implementation","text":"<p>Check the inference component documentation and run inference using different sentinel-2 products in parallel using calrissian or once in a time using cwltool.</p>"},{"location":"hands-on/#describing-the-ml-model","title":"Describing the ML Model","text":"<p>Refer to the MLM component documentation and execute the provided notebook to describe the machine learning model.</p>"},{"location":"inference-container/","title":"Inference container:","text":"<p>This module enables users to create an inference pipeline that take a Sentinel-2 STAC Item from the Planetary Computer, and generates a binary mask TIFF image using a pre-trained CNN model. For details on how the model was trained, refer to the training container documentation.</p>"},{"location":"inference-container/#make-inference-module","title":"Make Inference Module:","text":"<p>Inputs: - <code>input_reference</code>: The reference to a Sentinel-2 product on planetary computer. The application will give you an accurate result if the sentinel-2 product has no/low cloud-cover.</p> <p>Outputs:</p> <ul> <li><code>{STAC_ITEM_ID}_classified.tif</code>: A binary <code>.tif</code> image in <code>COG</code> format classifies:</li> </ul> Class ID Class Name 0 AnnualCrop 1 Forest 2 HerbaceousVegetation 3 Highway 4 Industrial 5 Pasture 6 PermanentCrop 7 Residential 8 River 9 SeaLake 10 No Data <ul> <li><code>overview_{STAC_ITEM_ID}_classified.tif</code>: A binary <code>.tif</code> image in <code>COG</code> format classifies:</li> </ul> Class ID Class Name 0 AnnualCrop 1 Forest 2 HerbaceousVegetation 3 Highway 4 Industrial 5 Pasture 6 PermanentCrop 7 Residential 8 River 9 SeaLake 10 No Data <ul> <li><code>STAC objects</code>: STAC objects related to the provided masks, including STAC catalog and STAC Item.</li> </ul>"},{"location":"inference-container/#how-the-application-works","title":"How the Application Works","text":"<p>The application begins by reading a Sentinel-2 STAC Item from the Planetary Computer. It then filters and selects 12 specific asset references in the order expected by the machine learning model. These assets correspond to common Sentinel-2 bands, as shown below:</p> Index Asset Key Asset Common Name 1 B01 Coastal 2 B02 Blue 3 B03 Green 4 B04 Red 5 B05 Red Edge 6 B06 Red Edge 7 B07 Red Edge 8 B08 NIR 9 B8A Narrow NIR 10 B09 Water Vapor 11 B11 SWIR 1 (16) 12 B12 SWIR 2 (22) <p>As a preprocessing step, all selected assets are resampled to a uniform resolution of 10 meters.</p> <p>The pipeline then proceeds with a sliding window approach: it reads and stacks small image chips from the selected bands in the order listed above. These chips are fed into a trained CNN model, which predicts the corresponding class for each chip.</p> <p>Finally, the application generates: - The classification prediction map (as a GeoTIFF mask) - A visual overview image - An updated STAC item containing metadata and references to the output files</p>"},{"location":"inference-cwl/","title":"Inference Module &amp; CWL Runner","text":"<p>In the training module, a CNN model was trained on the EuroSAT dataset to classify image chips into 10 different land use/land cover classes. The training workflow was tracked using MLflow.</p> <p>This Application Package provides a CWL document that performs inference by applying the trained model to unseen Sentinel-2 data in order to generate a classified image. The CWL document contains a single main workflow that executes one <code>CommandLineTool</code> step. It also supports parallel execution by accepting a list of Sentinel-2 references as input, making it suitable for running at scale on a Minikube cluster.</p> <p>To execute the application, users have the option to use either cwltool or Calrissian as the CWL runner.</p>"},{"location":"inference-cwl/#inputs","title":"Inputs:","text":"<ul> <li><code>input_reference</code>: A list of reference to a Sentinel-2 product on planetary computer. The application will give you an accurate result if the sentinel-2 product has no/low cloud-cover.</li> </ul>"},{"location":"inference-cwl/#how-to-execute-the-application-package","title":"How to Execute the Application Package","text":"<p>Before running the application with a CWL runner, make sure to download and use the latest version of the CWL document:</p> <pre><code>cd inference/app-package\nVERSION=$(curl -s https://api.github.com/repos/eoap/machine-learning-process/releases/latest | jq -r '.tag_name')\ncurl -L -o \"tile-sat-inference.cwl\" \\\n  \"https://github.com/eoap/machine-learning-process/releases/download/${VERSION}/tile-sat-inference.${VERSION}.cwl\"\n</code></pre>"},{"location":"inference-cwl/#run-the-application-package","title":"Run the Application Package:","text":"<p>There are two methods to execute the application:</p> <ul> <li> <p>Executing the <code>tile-sat-inference</code> app using <code>cwltool</code>:</p> <pre><code>cwltool --podman --debug --parallel tile-sat-inference.cwl#tile-sat-inference params.yml\n</code></pre> </li> <li> <p>Executing the <code>tile-sat-inference</code> using <code>calrissian</code>:</p> <pre><code>calrissian --debug --stdout /calrissian/out.json --stderr /calrissian/stderr.log --usage-report /calrissian/report.json --max-ram 10G --max-cores 2 --parallel --tmp-outdir-prefix /calrissian/tmp/ --outdir /calrissian/results/ --tool-logs-basepath /calrissian/logs tile-sat-inference.cwl#tile-sat-inference params.yml\n</code></pre> <p>You can monitor the pod creation using command below:</p> <p><code>kubectl get pods</code> </p> </li> </ul>"},{"location":"inference-cwl/#how-the-cwl-document-designed","title":"How the CWL document designed:","text":"<p>The CWL file can be triggered using <code>cwltool</code> or <code>calrissian</code>. The user provides a <code>params.yml</code> file that passes all inputs needed by the CWL file to execute the module. The CWL file is designed to execute the module based on the structure below:</p> <p></p> <p><code>[]</code> in the image above indicates that the user may pass a list of parameters to the application package.</p> <p>The Application Package will generate a list of directories containing intermediate or final output. The number of folders containing a <code>{STAC_ITEM_ID}_classified.tif</code> and the corresponding STAC objects, such as STAC Catalog and STAC Item, depends on the number of input Sentinel-2 items.</p>"},{"location":"insights/","title":"Lessons Learned from Building a Machine Learning process for Geospatial Data","text":""},{"location":"insights/#introduction","title":"Introduction","text":"<p>This page highlights the technical challenges, design decisions, and key insights gained while developing the machine learning process for geospatial data pipeline. </p> <p>It also includes recommendations for future improvements and practical advice for replicating or extending the setup.</p>"},{"location":"insights/#design-decisions","title":"Design Decisions","text":""},{"location":"insights/#modular-workflow-templates","title":"Modular Workflow Templates","text":"<p>Decision: Separate the CWL execution, training pipeline, and inference pipeline  into distinct workflow templates.</p> <p>Outcome: * Enhanced reusability for other geospatial pipelines requiring similar preprocessing steps.</p>"},{"location":"insights/#stac-integration","title":"STAC Integration","text":"<p>Decision: Leverage the STAC API, Geoparquet, and DuckDB for querying and storing geospatial data.</p> <p>Outcome: * Improved interoperability with other geospatial tools and standards.</p>"},{"location":"insights/#tracking-the-process","title":"Tracking the process","text":"<p>Decision: Use MLFLOW exclusively for tracking the process of training workflow and selecting the best model candidate.</p>"},{"location":"insights/#test-inference-with-sentinel-2-product","title":"Test inference with Sentinel-2 product","text":"<p>Decision: Use Stars tool to stage-in a sentinel-2 product ready to pass to inference module.</p>"},{"location":"insights/#challenges-and-solutions","title":"Challenges and Solutions","text":""},{"location":"insights/#build-docker-images","title":"Build Docker Images","text":"<p>Challenge: Initially, we used an advanced tooling technique that leveraged Taskfile to build a Kaniko-based image and reference the CWL files. The image was then pushed to ttl.sh, a temporary image registry. This will help us to execute the application packages using calrissian. However, this process was slow and hard to debug, often failing due to the large size of the Kaniko images.</p> <p>Solution: We now push the Docker images to a dedicated GitHub Container Registry.</p>"},{"location":"mlm/","title":"Describes a trained machine learning model","text":"<p>This Item describe a trained machine learning model using MLM STAC extension. The STAC Machine Learning Model (MLM) Extension provides a standard set of fields to describe machine learning models trained on overhead imagery and enable running model inference.</p> <p>The main objectives of the extension are:</p> <ul> <li>to enable building model collections that can be searched alongside associated STAC datasets</li> <li>record all necessary bands, parameters, modeling artifact locations, and high-level processing steps to deploy an inference service.</li> </ul> <p>For additional information please follow this Describe-MLmodel.ipynb notebook.</p>"},{"location":"mlm/#for-developers","title":"For developers:","text":"<p>To run the notebook successfully, you must install the dependencies with hatch:</p> <pre><code>hatch shell prod\nhatch -e prod run python -m ipykernel install --user --name=mlm --display-name \"mlm\"\n</code></pre>"},{"location":"packages/","title":"Application Packages","text":"<p>This tutorial provides two separate application packages:</p> <ul> <li><code>training</code> </li> <li><code>inference</code></li> </ul> <p>Each application package has its own Docker image, which has been published to a dedicated GitHub Container Registry.</p> <p>For more details on how each package works, refer to the documentation for training and inference.</p>"},{"location":"training-container/","title":"Training a Machine Learning Model- Container","text":"<p>This tutorial containing a python application for training a deep learning model on EuroSAT dataset for tile-based classification task and employs MLflow for monitoring the ML model development cycle. MLflow is a crucial tool that ensures effective log tracking and preserves key information, including specific code versions, datasets used, and model hyperparameters. By logging this information, the reproducibility of the work drastically increases, enabling users to revisit and replicate past experiments accurately. Moreover, quality metrics such as classification accuracy, loss function fluctuations, and inference time are also tracked, enabling easy comparison between different models. The dataset used in this project consists of Sentinel-2 satellite images labeled with corresponding land use and cover categories. It provides a comprehensive representation of various land features. The dataset comprises 27,000 labeled and geo-referenced images, divided into 10 distinct classes. The multi-spectral version of the dataset includes all 13 Sentinel-2 bands, which retains the original value range of the Sentinel-2 bands, enabling access to a more comprehensive set of spectral information. You can find the dataset on a dedicated STAC endpoint. </p> <p></p>"},{"location":"training-container/#inputs","title":"Inputs","text":"<p>This application supports training the CNN model using either CPU or GPU to accelerate the process. It accepts the following input parameters:</p> Parameter Type Default Value Description stac_reference <code>str</code> <code>https://raw.githubusercontent.com/eoap/machine-learning-process/main/training/app-package/EUROSAT-Training-Dataset/catalog.json</code> URL pointing to a STAC catalog. The model reads GeoParquet annotations from the collection's assets. BATCH_SIZE <code>int</code> <code>2</code> Number of batches CLASSES <code>int</code> <code>10</code> Number of land cover classes to classify. DECAY <code>float</code> <code>0.1</code> Decay value used in training. EPOCHS<code>|</code>int` required Number of epochs EPSILON<code>|</code>float<code>|</code>1e-6` epsilon value (Model's heyperparameter) LEARNING_RATE<code>|</code>float<code>|</code>0.0001` Initial learning rate for the optimizer. LOSS <code>str</code> <code>categorical_crossentropy</code> Loss function for training. Options: <code>binary_crossentropy</code>, <code>cosine_similarity</code>, <code>mean_absolute_error</code>, <code>mean_squared_logarithmic_error</code>, <code>squared_hinge</code>. MEMENTUM <code>float</code> <code>0.95</code> Momentum parameter used in optimizers OPTIMIZER <code>str</code> <code>Adam</code> Optimization algorithm. Options: <code>Adam</code>, <code>SGD</code>, <code>RMSprop</code>. REGULARIZER <code>str</code> <code>None</code> Regularization technique to avoid overfitting. Options: <code>l1l2</code>,<code>l1</code>, <code>l2</code>, <code>None</code>. SAMPLES_PER_CLASS <code>int</code> <code>10</code> Number of samples to use for training per class."},{"location":"training-container/#outputs","title":"Outputs:","text":"<ul> <li><code>mlruns</code>: Directory containing artifacts, metrics, and metadata for each training run, tracked and organized by MLflow.</li> </ul>"},{"location":"training-container/#how-the-application-structured-internally","title":"How the application structured internally","text":"<p>The training pipeline for developing the training module encompasses 4 main components including:</p> <ul> <li> <p>Data Ingestion</p> </li> <li> <p>Based Model Architecture</p> </li> <li> <p>Training</p> </li> <li> <p>Evaluation</p> </li> </ul> <p>The pipeline for this task is illustrated in the diagram below:</p> <p></p>"},{"location":"training-container/#data-ingestion","title":"Data Ingestion","text":"<p>This component is designed to fetch data from a dedicated STAC endpoint containing a collection of STAC Items representing EuroSAT image chips. The user can query the collection using DuckDB on a GeoParquet file and split the resulting data into training, validation, and test datasets.</p>"},{"location":"training-container/#based-model-architecture","title":"Based Model Architecture","text":"<p>In this component, the user will design a CNN based model with 7 layers. The first layer serves as the input layer, accepting an image with a shape of (13, 64, 64) or any other cubic image shapes (e.g. (3,64,64)). This is followed by 4 convolutional layers, each employing a relu activation function, a BatchNormalization layer,a 2D MaxPooling operation, and a Dropout layer. Subsequently, the model includes a Dense layer, and finally, the output layer generates a vector with 10 cells. Notably, the output layer utilizes the softmax activation function to produce the probabilities associated with each class. The user will choose a loss function, and an optimizer among available loss functions and optimizers. Eventually, the model is compiled and located under <code>output/prepare_based_model</code>.</p>"},{"location":"training-container/#training","title":"Training","text":"<p>This component is responsible for training the model for a specified number of epochs, as provided by the user through the application package inputs.</p>"},{"location":"training-container/#evaluation","title":"Evaluation","text":"<p>The user can evaluate the trained model, and MLflow will track the process for each run under a designated experiment. Once the MLflow service is deployed and running on port <code>5000</code>, the UI can be accessed at http://localhost:5000.</p> <p>MLflow tracks the following:</p> <ul> <li>Evaluation metrics, including <code>Accuracy</code>, <code>Precision</code>, <code>Recall</code>, and the loss value  </li> <li>Trained machine learning model saved after each run  </li> <li>Additional artifacts, such as:</li> <li>Loss curve plot during training  </li> <li>Confusion matrix</li> </ul>"},{"location":"training-container/#for-developers","title":"For developers","text":"<ol> <li><code>src</code>/ <code>tile_based_training</code> /<ul> <li>components /<ul> <li>Containing all components such as data_ingestion.py, prepare_base_model.py, train_model.py , model_evaluation.py, inference.py.</li> </ul> </li> <li>config /<ul> <li>Containing all configuration needed for each component.</li> </ul> </li> <li>utils /<ul> <li>to define helper functions.</li> </ul> </li> <li>pipeline /<ul> <li>to define the order of executing for each component.</li> </ul> </li> </ul> </li> </ol>"},{"location":"training-cwl/","title":"Training Module &amp; CWL Runner","text":"<p>This Application Package provides a CWL document containing a top-level workflow with a singleCommandLineToolstep that executes the training pipeline. It also supports parallel execution, allowing users to specify multiple sets of hyperparameter or training configurations. This makes it suitable for large-scale experiments and hyperparameter tuning on platforms like a Minikube cluster.</p> <p>To execute the training workflow, users can choose between cwltool and Calrissian as their CWL runners.</p>"},{"location":"training-cwl/#inputs","title":"Inputs:","text":"Parameter Type Description MLFLOW_TRACKING_URI string An environment variable for the MLFLOW_TRACKING_URI stac_reference string URL pointing to a STAC catalog. The model reads GeoParquet annotations from the collection's assets. BATCH_SIZE int[] Number of batches CLASSES int Number of land cover classes to classify. DECAY float[] Decay value used in training. EPOCHS int[] Number of epochs EPSILON float[] Epsilon value (model hyperparameter) LEARNING_RATE float[] Initial learning rate for the optimizer. LOSS string[] Loss function for training. Options: <code>binary_crossentropy</code>, <code>cosine_similarity</code>, <code>mean_absolute_error</code>, <code>mean_squared_logarithmic_error</code>, <code>squared_hinge</code>. MEMENTUM float[] Momentum parameter used in optimizers OPTIMIZER string[] Optimization algorithm. Options: <code>Adam</code>, <code>SGD</code>, <code>RMSprop</code>. REGULARIZER string[] Regularization technique to avoid overfitting. Options: <code>l1l2</code>, <code>l1</code>, <code>l2</code>, <code>None</code>. SAMPLES_PER_CLASS int Number of samples to use for training per class."},{"location":"training-cwl/#how-to-execute-the-application-package","title":"How to execute the application-package?","text":"<p>Before running the application with a CWL runner, make sure to download and use the latest version of the CWL document: <pre><code>cd training/app-package\nVERSION=$(curl -s https://api.github.com/repos/eoap/machine-learning-process/releases/latest | jq -r '.tag_name')\ncurl -L -o \"tile-sat-training.cwl\" \\\n  \"https://github.com/eoap/machine-learning-process/releases/download/${VERSION}/tile-sat-training.${VERSION}.cwl\"\n</code></pre></p>"},{"location":"training-cwl/#run-the-application-package","title":"Run the Application package:","text":"<p>There are two methods to execute the application:</p> <ul> <li>Executing the tile-based-training using cwltool in a terminal:</li> </ul> <pre><code> cwltool --podman --debug --parallel tile-sat-training.cwl#tile-sat-training params.yml\n</code></pre> <ul> <li>Executing the tile-based classification using calrissian in a terminal:</li> </ul> <pre><code> calrissian --debug --stdout /calrissian/out.json --stderr /calrissian/stderr.log --usage-report /calrissian/report.json --parallel --max-ram 10G --max-cores 2 --tmp-outdir-prefix /calrissian/tmp/ --outdir /calrissian/results/ --tool-logs-basepath /calrissian/logs tile-sat-training.cwl#tile-sat-training params.yaml\n</code></pre> <p>You can monitor the pod creation using command below:</p> <p><code>kubectl get pods</code> </p>"},{"location":"training-cwl/#how-the-cwl-document-designed","title":"How the CWL document designed:","text":"<p>The CWL file can be triggered using <code>cwltool</code> or <code>calrissian</code>. The user provides a <code>params.yml</code> file that passes all inputs needed by the CWL file to execute the module. The CWL file is designed to execute the module based on the structure below:</p> <p></p> <p><code>[]</code> in the image above indicates that the user may pass a list of parameters to the application package.</p>"}]}